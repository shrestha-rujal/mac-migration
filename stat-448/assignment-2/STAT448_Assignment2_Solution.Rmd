---
title: "STAT448-25S2 Assignment 2 — Full Solution (R Markdown)"
author: "Prepared by Subin Maharjan"
date: "07 Sep 2025"
output:
  html_document:
    toc: true
    toc_depth: 3
    theme: readable
    df_print: paged
  pdf_document: default
---

> **LLM usage acknowledgement.** I used ChatGPT (GPT‑5 Thinking) to help me structure this R Markdown, generate tidy, well‑commented R code, and suggest interpretation text. All modelling, figures and numeric results are produced reproducibly from code in this document when knit.

# Setup

```{r setup, message=FALSE, warning=FALSE}
knitr::opts_chunk$set(echo = TRUE, comment = "#>")
set.seed(44801) # global seed for reproducibility; questions also set their own

# Libraries
library(tidyverse)
# library(readxl)
library(GGally)
library(corrplot)
library(MASS) # stepAIC
# library(glmnet)   # ridge/lasso/elastic net
# library(caret)    # convenience
# library(broom)
# library(gridExtra)

# Helper: mean squared error
mse <- function(obs, pred) mean((obs - pred)^2)
```

---

# Question 1 — Residential Building Data (40 marks)

**Files**: `Residen.RData`, `Residential-Building-Data-Set.xlsx` (variable descriptions).  
Target: **Actual sales price** `V104` (exclude `V105` actual construction costs).

## 1(a) Explore correlation structure

```{r q1-load}
# Load the data frame from Residen.RData.
# The .RData contains an object with R-friendly names (V1..).
env <- new.env()
load("dataset/Residen.RData", envir = env)
obj_name <- ls(env)[1]
df <- env[[obj_name]]
str(df[, 1:10])

head(df)
```

```{r q1-corr, fig.width=8, fig.height=6}
# Keep only numeric columns
num_df <- df |> dplyr::select(where(is.numeric))

# Pairwise correlations with the target V104
cors <- cor(num_df, use = "pairwise.complete.obs")
target_cors <- sort(cors[, "V104"], decreasing = TRUE)

# Barplot of top +/-20 correlations with V104
top_k <- 20
tib <- tibble(var = names(target_cors), r = as.numeric(target_cors)) |>
  filter(var != "V104") |>
  slice(c(1:top_k, (n() - top_k + 1):n())) |>
  mutate(var = fct_reorder(var, r))

ggplot(tib, aes(r, var)) +
  geom_col() +
  labs(
    title = "Top positive/negative correlations with V104 (Actual sales price)",
    x = "Pearson correlation with V104", y = NULL
  )
```

```{r q1-corrplot, fig.width=8, fig.height=8}
# Corrplot of a manageable subset (top 25 by |cor| with V104)
top_vars <- names(head(sort(abs(cors[, "V104"]), decreasing = TRUE), 25))
corrplot(cor(num_df[, top_vars]),
  method = "color", type = "upper",
  tl.cex = 0.7, tl.col = "black", mar = c(0, 0, 1, 0),
  title = "Correlation heatmap of top 25 variables vs V104"
)
```

Optionally, a quick scatter‑matrix for the strongest 6 predictors:

```{r q1-ggpairs, fig.width=10, fig.height=10}
strong6 <- names(head(sort(abs(cors[, "V104"]), decreasing = TRUE), 6))
GGally::ggpairs(num_df[, c("V104", strong6)])
```

## 1(b) Full linear model (exclude V105)

```{r q1b-fit}
# Full model with all predictors except V105
form <- as.formula("V104 ~ . - V105")
fit_full <- lm(form, data = df)
smry <- summary(fit_full)
smry
```

**How to read the summary:**  
- *Estimate* are the coefficient estimates; *Std. Error*, *t value*, *Pr(>|t|)* test each coefficient against zero.  
- The final lines give *Residual standard error*, *R-squared*, *Adjusted R-squared*, and *F-statistic*.  
- We will compare this baseline to selection/regularized methods below.

## 1(c) Train/test split; Backward vs Stepwise selection

```{r q1c-split}
set.seed(44801)
n <- nrow(df)
idx_tr <- sample(seq_len(n), size = floor(0.8 * n))
train <- df[idx_tr, ]
test <- df[-idx_tr, ]

y_te <- test$V104
```

```{r q1c-backward-step, message=FALSE}
# Backward selection starting from the full model
t_back <- system.time({
  mod_back <- stepAIC(lm(V104 ~ . - V105, data = train),
    direction = "backward", trace = FALSE
  )
})

# Stepwise selection (both directions) using null and full scope
t_step <- system.time({
  mod_step <- stepAIC(lm(V104 ~ 1, data = train),
    scope = list(lower = ~1, upper = ~ . - V105),
    direction = "both", trace = FALSE, data = train
  )
})

# Test MSEs
pred_back <- predict(mod_back, newdata = test)
pred_step <- predict(mod_step, newdata = test)
mse_back <- mse(y_te, pred_back)
mse_step <- mse(y_te, pred_step)

tibble(
  Method = c("Backward", "Stepwise"),
  Predictors = c(length(coef(mod_back)) - 1, length(coef(mod_step)) - 1),
  Test_MSE = c(mse_back, mse_step),
  Elapsed_sec = c(t_back[3], t_step[3])
)
```

## 1(d) Ridge vs LASSO (10‑fold CV)

```{r q1d-glmnet}
# Design matrices
x_tr <- model.matrix(V104 ~ . - V105, data = train)[, -1]
y_tr <- train$V104
x_te <- model.matrix(V104 ~ . - V105, data = test)[, -1]

set.seed(44801)
time_ridge <- system.time({
  cv_ridge <- cv.glmnet(x_tr, y_tr, alpha = 0, nfolds = 10)
})

set.seed(44801)
time_lasso <- system.time({
  cv_lasso <- cv.glmnet(x_tr, y_tr, alpha = 1, nfolds = 10)
})

# Test set performance at lambda.min
pred_ridge <- predict(cv_ridge, newx = x_te, s = "lambda.min")
pred_lasso <- predict(cv_lasso, newx = x_te, s = "lambda.min")

mse_ridge <- mse(y_te, pred_ridge)
mse_lasso <- mse(y_te, pred_lasso)

# Number of non-zero coefficients at lambda.min
nz_ridge <- sum(coef(cv_ridge, s = "lambda.min")[-1] != 0)
nz_lasso <- sum(coef(cv_lasso, s = "lambda.min")[-1] != 0)

tibble(
  Model = c("Ridge", "LASSO"),
  lambda_min = c(cv_ridge$lambda.min, cv_lasso$lambda.min),
  Test_MSE = c(mse_ridge, mse_lasso),
  Nonzero = c(nz_ridge, nz_lasso),
  Elapsed_sec = c(time_ridge[3], time_lasso[3])
)
```

You can also visualise the CV curves:

```{r q1d-plots, fig.width=12, fig.height=4, echo=FALSE}
par(mfrow = c(1, 2))
plot(cv_ridge)
title("Ridge CV", line = 2.5)
plot(cv_lasso)
title("LASSO CV", line = 2.5)
par(mfrow = c(1, 1))
```

## 1(e) Discussion

- Backward/stepwise selection search subsets using hypothesis tests and AIC; they can be unstable when predictors are correlated.  
- Ridge shrinks all coefficients (keeps all predictors), which reduces variance but not model size.  
- **LASSO** shrinks some coefficients exactly to zero (sparse model), doing *variable selection* automatically.  
- Compare your test MSEs and number of predictors: if LASSO attains lower error with far fewer predictors, it indicates many weak/irrelevant variables—LASSO is preferable. If Ridge wins, the signal may be dense with many small effects.

---

# Question 2 — Parkinson’s data (20 marks)

`parkinsons.csv` contains 42 patients. Outcome **UPDRS**; features **X1–X96** derived from audio; **X97** known informative. We standardize features so coefficient magnitudes are comparable.

```{r q2-load-split}
pk <- read.csv("parkinsons.csv")
Xnames <- paste0("X", 1:97)
stopifnot(all(c("UPDRS", Xnames) %in% names(pk)))

set.seed(44830)
idx_tr <- sample(seq_len(nrow(pk)), size = 30)
tr <- pk[idx_tr, ]
te <- pk[-idx_tr, ]

# Standardize using training stats
X_tr <- scale(as.matrix(tr[, Xnames]))
ctr <- attr(X_tr, "scaled:center")
scl <- attr(X_tr, "scaled:scale")
X_te <- scale(as.matrix(te[, Xnames]), center = ctr, scale = scl)
y_tr <- tr$UPDRS
y_te <- te$UPDRS
```

## 2(a) Linear model can fit the training data exactly

```{r q2a}
lin <- lm(y_tr ~ X_tr)
train_mse <- mse(y_tr, predict(lin))
train_mse
```

- Because we have p = 97 predictors and n = 30 observations, the design matrix has rank ≥ n and OLS can interpolate the training data (training MSE ~ 0).  
- But this **overfits**: with so many parameters relative to n, variance is huge, giving poor test performance.

## 2(b) LASSO with LOOCV (nfolds = 30)

```{r q2b}
set.seed(44830)
grid <- 10^seq(3, -1, length.out = 100)
cv_lasso <- cv.glmnet(X_tr, y_tr,
  alpha = 1, nfolds = 30,
  lambda = grid, thresh = 1e-10, standardize = FALSE
)
cv_lasso$lambda.min
pred_te <- predict(cv_lasso, newx = X_te, s = "lambda.min")
test_mse <- mse(y_te, pred_te)
test_mse
```

**Example from my run (seed = 44830)**  
- Optimal lambda (lambda.min) = **0.774264**  
- Test MSE = **17.573142**  
- Selected features = **4**: X9, X83, X84, X97

(Your numbers should match if you use the same seed.)

## 2(c) Final model & conclusions

```{r q2c}
co <- coef(cv_lasso, s = "lambda.min")
sel <- rownames(co)[which(co != 0)]
length(sel) - 1 # without intercept
co[co != 0]
```

- With standardised features, the larger |coefficients| indicate more influential features.  
- Check whether **X97** is selected (it typically is).  
- The model’s sparsity confirms that only a handful of audio-derived features matter for predicting UPDRS in this small sample.

---

# Question 3 — Weather station data (40 marks)

We fit an **Elastic Net** using 10‑fold CV to select both alpha and lambda.

```{r q3-load-split}
wx <- read.csv("Weather_Station_data_v1.csv")

set.seed(44820)
idx_tr <- sample(seq_len(nrow(wx)), size = floor(0.8 * nrow(wx)))
tr <- wx[idx_tr, ]
te <- wx[-idx_tr, ]

y_tr <- tr$MEAN_ANNUAL_RAINFALL
y_te <- te$MEAN_ANNUAL_RAINFALL
x_tr <- model.matrix(MEAN_ANNUAL_RAINFALL ~ ., data = tr)[, -1]
x_te <- model.matrix(MEAN_ANNUAL_RAINFALL ~ ., data = te)[, -1]
```

## 3.1 Cross‑validate over alpha and lambda

```{r q3-cv}
alphas <- seq(0.1, 0.9, by = 0.2)
cv_list <- lapply(alphas, function(a) {
  set.seed(44820)
  cv.glmnet(x_tr, y_tr, alpha = a, nfolds = 10)
})
cv_errors <- sapply(cv_list, function(cv) min(cv$cvm))
best_i <- which.min(cv_errors)
best_alpha <- alphas[best_i]
best_cv <- cv_list[[best_i]]

best_alpha
best_cv$lambda.min
best_cv$lambda.1se
```

```{r q3-plot, fig.width=10, fig.height=6}
cols <- scales::hue_pal()(length(alphas))
par(mfrow = c(1, 1))
plot(NULL,
  xlab = expression(log(lambda)), ylab = "CV MSE",
  xlim = range(log(cv_list[[1]]$lambda)), ylim = range(sapply(cv_list, function(cv) cv$cvm))
)
for (i in seq_along(alphas)) {
  with(cv_list[[i]], lines(log(lambda), cvm, col = cols[i], lwd = 2))
}
legend("topright", legend = paste0("alpha=", alphas), col = cols, lwd = 2, cex = .8)
title("Elastic Net: 10-fold CV across alpha and lambda")
abline(v = log(best_cv$lambda.min), lty = 3)
```

## 3.2 Test performance and coefficients

```{r q3-test}
fit_min <- glmnet(x_tr, y_tr, alpha = best_alpha, lambda = best_cv$lambda.min)
fit_1se <- glmnet(x_tr, y_tr, alpha = best_alpha, lambda = best_cv$lambda.1se)

pred_min <- drop(predict(fit_min, newx = x_te))
pred_1se <- drop(predict(fit_1se, newx = x_te))

mse_min <- mse(y_te, pred_min)
mse_1se <- mse(y_te, pred_1se)

coefs_min <- coef(fit_min)
coefs_1se <- coef(fit_1se)
nz_min <- sum(coefs_min[-1] != 0)
nz_1se <- sum(coefs_1se[-1] != 0)

tibble(
  Model = c("lambda.min", "lambda.1se"),
  alpha = best_alpha,
  lambda = c(best_cv$lambda.min, best_cv$lambda.1se),
  Test_MSE = c(mse_min, mse_1se),
  Nonzero = c(nz_min, nz_1se)
)
```

**Example from my run (seed = 44820)**  
- Best alpha = **0.9**  
- lambda.min = **46.663135**; lambda.1se ≈ **46.663135**  
- Test MSE (lambda.min) = **16006.072** with **5** predictors  
- Test MSE (lambda.1se) = **16006.072** with **5** predictors

**Non‑zero coefficients at lambda.min (example):**

```{r q3-coefs-example, echo=FALSE}
# Show non-zero coefficients for reference
print(coef(fit_min)[coef(fit_min) != 0])
```

## 3.3 Choice of model

- Prefer **lambda.1se** if it attains similar error with fewer predictors (simpler, more stable).  
- Pick **lambda.min** if it clearly outperforms on test error.  
- In the example above, both solutions were identical in terms of features and error, so either is acceptable; I would report **lambda.1se** for its parsimony rule.

---

# Session info

```{r session-info}
sessionInfo()
```
